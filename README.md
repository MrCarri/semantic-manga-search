# Semantic Manga Search

**A fullyâ€‘local, openâ€‘source Retrievalâ€‘Augmented Generation (RAG) pipeline** that crawls Mangadex, embeds manga metadata with MiniLMâ€‘L6â€‘v2, stores vectors in ChromaDB and uses Ollama (llama3â€¯8B / mistralâ€¯7Bâ€‘instruct) to filter and generate **five ranked manga recommendations**.  
All components run on a single RTXâ€¯2060 (â‰¤â€¯6â€¯GB VRAM).
Any model that is available on ollama can be used, but I recommend it to have at least 7B parameters for acceptable results. Also applicable to embedding model.

[Article I've written on the whole project available on my personal blog](https://mrcarri.dev/posts/software/llm-rag/)
---  

## Features
- **Local only** â€“ no external APIs besides the public Mangadex API (readâ€‘only).  
- **Openâ€‘source stack**: ChromaDB (Apacheâ€¯2.0), MiniLMâ€‘L6â€‘v2 (Apacheâ€¯2.0), Ollama (MIT).  
- **GPUâ€‘friendly**: fits comfortably on an RTXâ€¯2060 (â‰¤â€¯6â€¯GB VRAM).  
- **Deterministic embeddings** â€“ same query â†’ same vector.  

---  

## Architecture Overview

```text
[ Crawler ] --> [ JSONL Dataset ] --> [ MiniLM-L6 Embedding ]
                                              |
                                              v
[ User Query ] --> [ Embedding ] ------> [ ChromaDB (Vector Store) ]
                                              |
                                              v
[ Final Response ] <--- [ Ollama LLM ] <--- [ Top-15 Context ]
``` 

---  

## Prerequisites
| Requirement | How to install |
|-------------|----------------|
| **Git** | `sudo apt-get install git` (Linux) / `brew install git` (macOS) |
| **Pythonâ€¯â‰¥â€¯3.10** | `sudo apt-get install python3 python3â€‘venv` |
| **Ollama** (local LLM server) | Follow the official guide: <https://ollama.com/download> |
| **CUDA Toolkit** (for GPU inference) | `sudo apt-get install nvidia-cuda-toolkit` (Linux) |

---  

## Installation
```bash
# Clone the repository
git clone https://github.com/<TU_USUARIO>/semantic-manga-search.git
cd semantic-manga-search

# Create a virtual environment
python -m venv .venv && source .venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt

# Pull the LLM models (requires Ollama to be running)
# You can use whatever model you want. I recommend using one that has at least 7B parameters.
ollama pull llama3:8b
ollama pull mistral:7b-instruct-q4_K_M
``` 
## Quickâ€‘Start

```bash
# Crawl Mangadex (only manga tagged as "Mystery")
python scripts/crawler.py --included-tags "Mystery"

# Index the downloaded JSONL into ChromaDB
python scripts/indexer.py \
    --data-file data/mangas_index.jsonl \
    --db-path ./chroma_db

# Run a query
python scripts/query.py "realistic medical mystery manga with a doctor"
You should see five manga titles with a short justification printed on the console.
```

## Usage

All three scripts expose a standard â€‘h/--help interface. Below is a summary of the most useful flags; run script.py -h for the complete list.

### crawler.py CLI Arguments

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| --included-tags | str | "Mystery" | Comma-separated list of tags to include in the Mangadex API query. |
| --excluded-tags | str | "" | Comma-separated list of tags to exclude from the Mangadex API query. |

### indexer.py CLI Arguments

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| --data-file | Path | mangas_index.jsonl | Path to the .jsonl file generated by the crawler. |
| --db-path | str | "./manga_rag_db" | Path to the directory where ChromaDB will store the data. |
| --collection | str | "mystery_manga" | Name of the collection inside ChromaDB. |
| --embedding-model | str | "all-MiniLM-L6-v2" | The sentence-transformers model used to vectorize the text. |

### query.py CLI Arguments

| Flag / Argument | Type | Default | Description |
|-----------------|------|---------|-------------|
| query | str | Required | The natural language prompt describing the manga you are looking for. |
| --db-path | str | "./manga_rag_db" | Path to the directory where ChromaDB is stored. |
| --collection | str | "mystery_manga" | Name of the ChromaDB collection to query. |
| --limit | int | 15 | Number of documents to retrieve from the DB as context for the LLM. |
| --model | str | "llama3:8b" | The Ollama model name to use for reasoning. |
| --temperature | float | 0.2 | Controls randomness: 0.0 is deterministic, 1.0 is creative. |
| --ollama-url | str | ".../api/generate" | The API endpoint for your local Ollama server. |
| --embedding-model | str | "all-MiniLM-L6-v2" | The model used to vectorize your query (must match the indexer). |
| --num-predict | int | 800 | Maximum number of tokens the LLM is allowed to generate. |


## Configuration
All hyperâ€‘parameters are exposed via CLI flags (see the table above).
If you want to change the prompt sent to the LLM, edit the constant BASE_PROMPT in scripts/query.py.


## Limitations

- Result quality depends heavily on the completeness of the tag metadata and synopsis quality.
- No structured preâ€‘filter for prohibited words yet (currently handled by the LLM).
- Only English/Japanese titles are supported; no multilingual handling.


## Benchmarks (RTX 2060)
| Task | Time |
|------|------|
| Indexing (9k titles) | ~33s |
| LLM Reasoning (Llama 3) | ~16s |

## Contributing
Contributions are welcome! 
Please respect the existing code style (PEPâ€¯8) and update the README/CHANGELOG if you add new functionality.

## License
This project is released under the MIT License. See the LICENSE file for details.

## Acknowledgements

- ChromaDB (Apacheâ€¯2.0) â€“ vector store.
- Sentenceâ€‘Transformers / MiniLMâ€‘L6â€‘v2 (Apacheâ€¯2.0).
- Ollama (MIT) â€“ local LLM serving.
- Mangadex API (CCâ€‘BYâ€‘NCâ€‘SA) â€“ source of manga metadata.


## References

- ChromaDB docs: https://www.trychroma.com/docs
- MiniLMâ€‘L6â€‘v2 (Sentenceâ€‘Transformers): https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
- Ollama model catalog: https://ollama.com/library
- Mangadex API: https://api.mangadex.org/docs


## Contact
If you have questions, open an issue or reach out via the Discussions tab on GitHub. Happy hacking! ðŸš€
