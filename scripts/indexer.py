import argparse
import json
from pathlib import Path

import chromadb
from sentence_transformers import SentenceTransformer

# --- Constants ---
BATCH_SIZE = 5000  # safe insertion for chroma.


def _parse_args() -> argparse.Namespace:
    """Parse command line arguments for the ChromaDB Ingester."""
    parser = argparse.ArgumentParser(
        description="MangaDex Indexer for ChromaDB. Processes JSONL files to generate and index embeddings.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # 1. Data file path
    parser.add_argument(
        "--data-file",
        type=Path,
        default=Path("mangas_index.jsonl"),
        help="Path to the .jsonl file generated by the crawler.",
    )
    # 2. Database path
    parser.add_argument(
        "--db-path",
        type=str,
        default="./manga_rag_db",
        help="Path to the ChromaDB directory (e.g., './my_data/db').",
    )
    # 3. Collection name (on chroma DB)
    parser.add_argument(
        "--collection",
        type=str,
        default="mystery_manga",
        help="Name of the ChromaDB collection to query.",
    )
    # 4. Embedding model
    parser.add_argument(
        "--embedding-model",
        type=str,
        default="all-MiniLM-L6-v2",
        help="Embedding model from sentence-transformes to be used.",
    )
    return parser.parse_args()


def _process(args: argparse.Namespace, embedding_model: SentenceTransformer):
    print("Processing data...")
    documents = []
    ids = []
    seen_ids = set()  # This is for efficiency to avoid indexing duplicates (O(1)), instead of searching on ids.

    metadata = []
    data_file = args.data_file
    client = chromadb.PersistentClient(path=args.db_path)
    collection = client.get_or_create_collection(
        name=args.collection,
        configuration={"hnsw": {"space": "cosine", "ef_construction": 200}},
    )

    print(f"Reading from {data_file}")
    try:
        with open(data_file, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)

                manga_id = data["id"]

                if manga_id in seen_ids:
                    continue
                seen_ids.add(manga_id)
                ids.append(manga_id)
                title = str(data.get("title") or "N/A")
                author = str(data.get("author") or "N/A")
                artist = str(data.get("artist") or "N/A")
                status = str(data.get("status") or "N/A")
                tags = ", ".join(data.get("tags", []))
                year = str(data.get("year") or "N/A")

                document = f"Author: {author} tags: {tags} year: {year} synopsis: {data['description']}"
                # Add mangas info to lists
                documents.append(document)
                metadata.append(
                    {
                        "title": title,
                        "author": author,
                        "artist": artist,
                        "status": status,
                        "tags": tags,
                        "year": year,
                    },
                )
    except Exception as exc:
        raise RuntimeError(
            f"Error reading from source. Original error message: {str(exc)}"
        ) from exc

    print(f"Loaded {len(documents)} documents for indexing")
    print("Encoding embeddings...")
    try:
        embeddings = embedding_model.encode(documents, normalize_embeddings=False)
    except Exception as exc:
        raise RuntimeError(
            f"Error encoding documents. Original error message: {str(exc)}"
        ) from exc
    print("Adding elements to collection")

    total_indexed = 0
    total_documents = len(documents)
    try:
        for i in range(0, total_documents, BATCH_SIZE):
            batch_end = i + BATCH_SIZE
            collection.add(
                ids=ids[i:batch_end],
                embeddings=embeddings[i:batch_end],
                documents=documents[i:batch_end],
                metadatas=metadata[i:batch_end],
            )
            total_indexed += len(ids[i:batch_end])

    except Exception as exc:
        raise RuntimeError(
            f"Error adding to collection. Original error message: {str(exc)}"
        ) from exc
    print("Finished indexing data into database.")
    total_count = collection.count()
    print(f"Total documents on collection '{args.collection}': {total_count}")


if __name__ == "__main__":
    args = _parse_args()
    print("--- Manga chromadb indexer initializing ---")

    print(f"Loading embedding model: {args.embedding_model}...")
    try:
        EMBEDDING_MODEL = SentenceTransformer(args.embedding_model)
    except Exception as exc:
        raise RuntimeError(
            f"Failed to load embedding model: {args.embedding_model}. Detail: {exc}"
        ) from exc
    _process(args=args, embedding_model=EMBEDDING_MODEL)
